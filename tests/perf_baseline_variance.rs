//! Baseline variance analysis and statistical confidence bands (bd-3ar8v.1.5).
//!
//! Validates that performance baselines have acceptable variance for truthful
//! progress claims. Provides statistical primitives (confidence intervals,
//! coefficient of variation, variance classification) and produces structured
//! JSONL evidence conforming to `pi.perf.baseline_variance.v1`.
//!
//! Run:
//! ```bash
//! cargo test --test perf_baseline_variance -- --nocapture
//! ```

#![allow(
    clippy::cast_precision_loss,
    clippy::cast_possible_truncation,
    clippy::doc_markdown,
    clippy::too_many_lines,
    clippy::float_cmp,
    dead_code
)]

use serde::{Deserialize, Serialize};
use serde_json::{Value, json};
use std::fmt::Write as _;
use std::path::{Path, PathBuf};

// ─── Schema Constants ───────────────────────────────────────────────────────

const BASELINE_VARIANCE_SCHEMA: &str = "pi.perf.baseline_variance.v1";
const BASELINE_VARIANCE_VERSION: &str = "1.0.0";
const EVIDENCE_ADJUDICATION_MATRIX_SCHEMA: &str = "pi.qa.evidence_adjudication_matrix.v1";

// ─── Statistical Primitives ─────────────────────────────────────────────────

/// Coefficient of variation thresholds for variance classification.
const CV_LOW_THRESHOLD: f64 = 0.05;
const CV_MEDIUM_THRESHOLD: f64 = 0.15;

/// Variance classification based on coefficient of variation.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
enum VarianceClass {
    /// CV <= 0.05: Highly reproducible measurements
    Low,
    /// 0.05 < CV <= 0.15: Acceptable for most claims
    Medium,
    /// CV > 0.15: Too noisy for reliable claims
    High,
}

impl VarianceClass {
    fn from_cv(cv: f64) -> Self {
        if cv <= CV_LOW_THRESHOLD {
            Self::Low
        } else if cv <= CV_MEDIUM_THRESHOLD {
            Self::Medium
        } else {
            Self::High
        }
    }

    const fn is_acceptable(self) -> bool {
        matches!(self, Self::Low | Self::Medium)
    }

    const fn as_str(self) -> &'static str {
        match self {
            Self::Low => "low",
            Self::Medium => "medium",
            Self::High => "high",
        }
    }
}

/// t-distribution critical values for common degrees of freedom.
/// Used for computing confidence intervals with small sample sizes.
const fn t_critical_95(df: usize) -> f64 {
    match df {
        1 => 12.706,
        2 => 4.303,
        3 => 3.182,
        4 => 2.776,
        5 => 2.571,
        6 => 2.447,
        7 => 2.365,
        8 => 2.306,
        9 => 2.262,
        10 => 2.228,
        11..=15 => 2.131,
        16..=20 => 2.086,
        21..=25 => 2.060,
        26..=30 => 2.042,
        31..=50 => 2.009,
        _ => 1.96, // Normal approximation for large df
    }
}

const fn t_critical_99(df: usize) -> f64 {
    match df {
        1 => 63.657,
        2 => 9.925,
        3 => 5.841,
        4 => 4.604,
        5 => 4.032,
        6 => 3.707,
        7 => 3.499,
        8 => 3.355,
        9 => 3.250,
        10 => 3.169,
        11..=15 => 2.947,
        16..=20 => 2.845,
        21..=25 => 2.787,
        26..=30 => 2.750,
        31..=50 => 2.678,
        _ => 2.576, // Normal approximation
    }
}

/// Confidence interval bounds.
#[derive(Debug, Clone, Serialize, Deserialize)]
struct ConfidenceInterval {
    lower: f64,
    upper: f64,
    t_critical: f64,
    standard_error: f64,
    width: f64,
    width_pct: f64,
}

/// Full statistical summary for a set of measurements.
#[derive(Debug, Clone, Serialize, Deserialize)]
struct VarianceStats {
    count: usize,
    min: f64,
    max: f64,
    mean: f64,
    stddev: f64,
    coefficient_of_variation: f64,
    variance_class: String,
    p50: f64,
    p95: f64,
    p99: f64,
    confidence_interval_95: ConfidenceInterval,
    confidence_interval_99: ConfidenceInterval,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct CrossEnvVarianceBreakdown {
    environment_count: usize,
    mean: f64,
    env_stddev: f64,
    spread_pct: f64,
    noise_floor_pct: f64,
    signal_to_noise: f64,
    environment_component_pct: f64,
    build_component_pct: f64,
    runtime_component_pct: f64,
    noise_component_pct: f64,
    dominant_source: String,
    alert_triggered: bool,
}

/// Compute full variance statistics for a sample.
fn compute_variance_stats(samples: &[f64]) -> Option<VarianceStats> {
    let n = samples.len();
    if n < 2 {
        return None;
    }

    let mut sorted = samples.to_vec();
    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));

    let sum: f64 = sorted.iter().sum();
    let mean = sum / n as f64;

    // Sample standard deviation (Bessel's correction: n-1)
    let variance = sorted.iter().map(|x| (x - mean).powi(2)).sum::<f64>() / (n - 1) as f64;
    let stddev = variance.sqrt();

    let cv = if mean.abs() > f64::EPSILON {
        stddev / mean.abs()
    } else {
        0.0
    };

    let var_class = VarianceClass::from_cv(cv);

    // Standard error of the mean
    let se = stddev / (n as f64).sqrt();

    // Degrees of freedom
    let df = n - 1;

    // 95% CI
    let t95 = t_critical_95(df);
    let ci95_lower = mean - t95 * se;
    let ci95_upper = mean + t95 * se;
    let ci95_width = ci95_upper - ci95_lower;
    let ci95_width_pct = if mean.abs() > f64::EPSILON {
        (ci95_width / mean.abs()) * 100.0
    } else {
        0.0
    };

    // 99% CI
    let t99 = t_critical_99(df);
    let ci99_lower = mean - t99 * se;
    let ci99_upper = mean + t99 * se;
    let ci99_width = ci99_upper - ci99_lower;
    let ci99_width_pct = if mean.abs() > f64::EPSILON {
        (ci99_width / mean.abs()) * 100.0
    } else {
        0.0
    };

    Some(VarianceStats {
        count: n,
        min: sorted[0],
        max: sorted[n - 1],
        mean,
        stddev,
        coefficient_of_variation: cv,
        variance_class: var_class.as_str().to_string(),
        p50: percentile(&sorted, 50.0),
        p95: percentile(&sorted, 95.0),
        p99: percentile(&sorted, 99.0),
        confidence_interval_95: ConfidenceInterval {
            lower: ci95_lower,
            upper: ci95_upper,
            t_critical: t95,
            standard_error: se,
            width: ci95_width,
            width_pct: ci95_width_pct,
        },
        confidence_interval_99: ConfidenceInterval {
            lower: ci99_lower,
            upper: ci99_upper,
            t_critical: t99,
            standard_error: se,
            width: ci99_width,
            width_pct: ci99_width_pct,
        },
    })
}

fn compute_cross_env_variance_breakdown(
    env_means: &[f64],
    env_cvs_pct: &[f64],
    alert_threshold_pct: f64,
) -> Option<CrossEnvVarianceBreakdown> {
    if env_means.len() < 2 || env_means.len() != env_cvs_pct.len() {
        return None;
    }

    let n = env_means.len();
    let mean = env_means.iter().sum::<f64>() / n as f64;
    let variance = env_means.iter().map(|x| (x - mean).powi(2)).sum::<f64>() / (n - 1) as f64;
    let env_stddev = variance.sqrt();
    let spread_pct = if mean.abs() > f64::EPSILON {
        (env_stddev / mean.abs()) * 100.0
    } else {
        0.0
    };

    let noise_floor_pct = env_cvs_pct.iter().sum::<f64>() / n as f64;
    let signal_to_noise = spread_pct / noise_floor_pct.max(1e-9);

    let environment_component_pct = (spread_pct - noise_floor_pct).max(0.0);
    let noise_component_pct = spread_pct.min(noise_floor_pct);
    let build_component_pct: f64 = 0.0;
    let runtime_component_pct = noise_component_pct;

    let dominant_source = if environment_component_pct
        > build_component_pct.max(runtime_component_pct.max(noise_component_pct))
    {
        "environment"
    } else if runtime_component_pct > build_component_pct.max(noise_component_pct) {
        "runtime"
    } else if noise_component_pct > 0.0 {
        "noise"
    } else {
        "build"
    };

    Some(CrossEnvVarianceBreakdown {
        environment_count: n,
        mean,
        env_stddev,
        spread_pct,
        noise_floor_pct,
        signal_to_noise,
        environment_component_pct,
        build_component_pct,
        runtime_component_pct,
        noise_component_pct,
        dominant_source: dominant_source.to_string(),
        alert_triggered: spread_pct >= alert_threshold_pct,
    })
}

#[allow(clippy::cast_sign_loss)]
fn percentile(sorted: &[f64], p: f64) -> f64 {
    if sorted.is_empty() {
        return 0.0;
    }
    let idx = (p / 100.0) * (sorted.len() as f64 - 1.0);
    let lo = idx.floor() as usize;
    let hi = (lo + 1).min(sorted.len() - 1);
    let frac = idx - lo as f64;
    sorted[hi].mul_add(frac, sorted[lo] * (1.0 - frac))
}

fn project_root() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR"))
}

fn output_dir() -> PathBuf {
    let base = std::env::var("PERF_REGRESSION_OUTPUT")
        .ok()
        .map_or_else(|| project_root().join("target/perf"), PathBuf::from);
    let _ = std::fs::create_dir_all(&base);
    base
}

fn append_jsonl(path: &Path, line: &str) {
    use std::io::Write;
    let _ = std::fs::create_dir_all(path.parent().unwrap_or_else(|| Path::new(".")));
    let mut file = std::fs::OpenOptions::new()
        .create(true)
        .append(true)
        .open(path)
        .expect("open JSONL for append");
    let _ = writeln!(file, "{line}");
}

// ═══════════════════════════════════════════════════════════════════════════
// Tests: Statistical Primitive Validation
// ═══════════════════════════════════════════════════════════════════════════

#[test]
fn variance_class_low_for_tight_measurements() {
    // Tight measurements with very low variance
    let samples = vec![100.0, 100.1, 99.9, 100.05, 99.95];
    let stats = compute_variance_stats(&samples).unwrap();

    assert_eq!(stats.variance_class, "low");
    assert!(
        stats.coefficient_of_variation < CV_LOW_THRESHOLD,
        "CV {} should be < {}",
        stats.coefficient_of_variation,
        CV_LOW_THRESHOLD
    );
}

#[test]
fn variance_class_medium_for_moderate_spread() {
    // Moderate spread: ~10% variation
    let samples = vec![100.0, 105.0, 95.0, 102.0, 98.0, 107.0, 93.0];
    let stats = compute_variance_stats(&samples).unwrap();

    assert_eq!(
        stats.variance_class, "medium",
        "CV={} should classify as medium",
        stats.coefficient_of_variation
    );
}

#[test]
fn variance_class_high_for_noisy_data() {
    // Wide spread: >15% variation
    let samples = vec![100.0, 130.0, 70.0, 150.0, 80.0];
    let stats = compute_variance_stats(&samples).unwrap();

    assert_eq!(
        stats.variance_class, "high",
        "CV={} should classify as high",
        stats.coefficient_of_variation
    );
}

#[test]
fn confidence_interval_contains_true_mean() {
    // Generate samples from a known distribution
    // True mean = 100, with small noise
    let samples = vec![
        98.5, 101.2, 99.8, 100.5, 100.0, 99.3, 100.7, 99.9, 100.1, 100.3,
    ];
    let stats = compute_variance_stats(&samples).unwrap();

    // The 95% CI should contain the true mean (100.0) for well-behaved data
    assert!(
        stats.confidence_interval_95.lower <= 100.0 && stats.confidence_interval_95.upper >= 100.0,
        "95% CI [{}, {}] should contain true mean 100.0",
        stats.confidence_interval_95.lower,
        stats.confidence_interval_95.upper
    );
}

#[test]
fn confidence_interval_99_wider_than_95() {
    let samples = vec![100.0, 102.0, 98.0, 101.0, 99.0, 100.5, 99.5];
    let stats = compute_variance_stats(&samples).unwrap();

    assert!(
        stats.confidence_interval_99.width > stats.confidence_interval_95.width,
        "99% CI width {} should be wider than 95% CI width {}",
        stats.confidence_interval_99.width,
        stats.confidence_interval_95.width
    );
}

#[test]
fn small_sample_uses_t_distribution() {
    // With only 3 samples, t-critical should be much larger than z=1.96
    let samples = vec![100.0, 105.0, 95.0];
    let stats = compute_variance_stats(&samples).unwrap();

    assert!(
        stats.confidence_interval_95.t_critical > 2.0,
        "t-critical for df=2 should be > 2.0, got {}",
        stats.confidence_interval_95.t_critical
    );
    assert!(
        stats.confidence_interval_95.t_critical > 4.0,
        "t-critical for df=2 should be ~4.303, got {}",
        stats.confidence_interval_95.t_critical
    );
}

#[test]
fn coefficient_of_variation_is_correct() {
    // Known values: mean=10, stddev=2 → CV=0.2
    let samples = vec![8.0, 10.0, 12.0, 8.0, 10.0, 12.0, 8.0, 10.0, 12.0, 10.0];
    let stats = compute_variance_stats(&samples).unwrap();

    // CV should be approximately stddev/mean
    let expected_cv = stats.stddev / stats.mean;
    let diff = (stats.coefficient_of_variation - expected_cv).abs();
    assert!(
        diff < 1e-10,
        "CV {} should equal stddev/mean {}",
        stats.coefficient_of_variation,
        expected_cv
    );
}

#[test]
fn percentiles_are_monotonically_increasing() {
    let samples = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0];
    let stats = compute_variance_stats(&samples).unwrap();

    assert!(stats.min <= stats.p50, "min <= p50");
    assert!(stats.p50 <= stats.p95, "p50 <= p95");
    assert!(stats.p95 <= stats.p99, "p95 <= p99");
    assert!(stats.p99 <= stats.max, "p99 <= max");
}

#[test]
fn two_samples_minimum_produces_valid_stats() {
    let samples = vec![100.0, 200.0];
    let stats = compute_variance_stats(&samples);
    assert!(stats.is_some(), "should produce stats with 2 samples");

    let stats = stats.unwrap();
    assert_eq!(stats.count, 2);
    assert!(
        (stats.mean - 150.0).abs() < f64::EPSILON,
        "mean should be 150"
    );
}

#[test]
fn single_sample_returns_none() {
    let samples = vec![100.0];
    let stats = compute_variance_stats(&samples);
    assert!(
        stats.is_none(),
        "should return None for single sample (can't compute variance)"
    );
}

#[test]
fn cross_env_breakdown_detects_environment_dominated_spread() {
    let env_means = vec![100.0, 122.0, 118.0];
    let env_cvs_pct = vec![2.0, 2.2, 1.8];
    let stats = compute_cross_env_variance_breakdown(&env_means, &env_cvs_pct, 10.0)
        .expect("breakdown should compute");

    assert!(
        stats.environment_component_pct > stats.noise_component_pct,
        "environment component should dominate when spread exceeds noise floor"
    );
    assert_eq!(stats.dominant_source, "environment");
    assert!(stats.alert_triggered, "spread should breach 10% threshold");
}

#[test]
fn cross_env_breakdown_classifies_noise_when_spread_within_noise_floor() {
    let env_means = vec![100.0, 101.0, 99.0];
    let env_cvs_pct = vec![4.5, 4.8, 4.2];
    let stats = compute_cross_env_variance_breakdown(&env_means, &env_cvs_pct, 10.0)
        .expect("breakdown should compute");

    assert!(
        stats.noise_component_pct >= stats.environment_component_pct,
        "noise component should dominate when spread is inside noise floor"
    );
    assert!(
        matches!(stats.dominant_source.as_str(), "noise" | "runtime"),
        "dominant source should indicate noise-like variance"
    );
    assert!(
        !stats.alert_triggered,
        "small spread should not trigger alert"
    );
}

#[test]
fn cross_env_breakdown_threshold_gate_behavior() {
    let env_means = vec![100.0, 112.0, 110.0];
    let env_cvs_pct = vec![1.5, 1.7, 1.6];

    let low_threshold = compute_cross_env_variance_breakdown(&env_means, &env_cvs_pct, 5.0)
        .expect("breakdown should compute");
    let high_threshold = compute_cross_env_variance_breakdown(&env_means, &env_cvs_pct, 20.0)
        .expect("breakdown should compute");

    assert!(
        low_threshold.alert_triggered,
        "spread should trigger at low threshold"
    );
    assert!(
        !high_threshold.alert_triggered,
        "spread should not trigger at high threshold"
    );
}

#[test]
fn cross_env_breakdown_component_decomposition_conserves_spread() {
    let env_means = vec![100.0, 111.0, 109.0, 107.0];
    let env_cvs_pct = vec![2.0, 2.5, 2.2, 2.1];
    let stats = compute_cross_env_variance_breakdown(&env_means, &env_cvs_pct, 10.0)
        .expect("breakdown should compute");

    let reconstructed_spread = stats.environment_component_pct + stats.noise_component_pct;
    let spread_delta = (reconstructed_spread - stats.spread_pct).abs();
    assert!(
        spread_delta < 1e-9,
        "component decomposition should conserve spread: reconstructed={reconstructed_spread:.12}, spread={:.12}, delta={spread_delta:.12}",
        stats.spread_pct
    );
    assert!(
        (stats.runtime_component_pct - stats.noise_component_pct).abs() < 1e-9,
        "runtime component should track noise component in current decomposition model"
    );
    assert!(
        stats.build_component_pct.abs() < f64::EPSILON,
        "build component is currently modeled as zero"
    );
}

#[test]
fn cross_env_breakdown_zero_noise_floor_yields_finite_signal_ratio() {
    let env_means = vec![100.0, 125.0, 75.0];
    let env_cvs_pct = vec![0.0, 0.0, 0.0];
    let stats = compute_cross_env_variance_breakdown(&env_means, &env_cvs_pct, 10.0)
        .expect("breakdown should compute");

    assert!(
        stats.noise_floor_pct.abs() < f64::EPSILON,
        "noise floor should be exactly zero for zero-CV input"
    );
    assert!(
        stats.signal_to_noise.is_finite() && stats.signal_to_noise > 0.0,
        "signal-to-noise should remain finite and positive when noise floor is zero: {}",
        stats.signal_to_noise
    );
    assert_eq!(
        stats.dominant_source, "environment",
        "with zero noise floor and nonzero spread, environment should dominate"
    );
}

#[test]
fn cross_env_breakdown_rejects_invalid_input_shapes() {
    assert!(
        compute_cross_env_variance_breakdown(&[100.0], &[2.0], 10.0).is_none(),
        "single-environment input should fail closed"
    );
    assert!(
        compute_cross_env_variance_breakdown(&[100.0, 105.0], &[2.0], 10.0).is_none(),
        "length mismatch should fail closed"
    );
    assert!(
        compute_cross_env_variance_breakdown(&[], &[], 10.0).is_none(),
        "empty input should fail closed"
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// Tests: Inline Baseline Variance Analysis
// ═══════════════════════════════════════════════════════════════════════════

#[test]
fn inline_json_parse_variance_is_acceptable() {
    // Run multiple rounds of JSON parsing to measure variance
    let msg = r#"{"type":"host_call","id":"hc-1","method":"log","params":{"level":"info","message":"hello"}}"#;
    let rounds = 10;
    let iterations_per_round = 5000;
    let mut round_means: Vec<f64> = Vec::with_capacity(rounds);

    for _ in 0..rounds {
        let start = std::time::Instant::now();
        for _ in 0..iterations_per_round {
            let _: Value = serde_json::from_str(msg).unwrap();
        }
        let elapsed_us = start.elapsed().as_nanos() as f64 / 1000.0;
        let per_parse_us = elapsed_us / f64::from(iterations_per_round);
        round_means.push(per_parse_us);
    }

    let stats = compute_variance_stats(&round_means).unwrap();

    eprintln!("\n=== JSON Parse Variance Analysis ===");
    eprintln!("  Rounds:    {rounds}");
    eprintln!("  Mean:      {:.2}us/parse", stats.mean);
    eprintln!("  Stddev:    {:.2}us", stats.stddev);
    eprintln!("  CV:        {:.4}", stats.coefficient_of_variation);
    eprintln!("  Class:     {}", stats.variance_class);
    eprintln!(
        "  95% CI:    [{:.2}, {:.2}]us",
        stats.confidence_interval_95.lower, stats.confidence_interval_95.upper
    );

    // JSON parse should have low/medium variance in a controlled environment
    let var_class = VarianceClass::from_cv(stats.coefficient_of_variation);
    assert!(
        var_class.is_acceptable(),
        "JSON parse variance class '{}' (CV={:.4}) should be acceptable (low or medium)",
        stats.variance_class,
        stats.coefficient_of_variation
    );

    // Emit structured evidence
    let record = json!({
        "schema": BASELINE_VARIANCE_SCHEMA,
        "metric": "json_parse_latency_us",
        "stats": stats,
        "rounds": rounds,
        "iterations_per_round": iterations_per_round,
        "acceptable": var_class.is_acceptable(),
    });
    append_jsonl(
        &output_dir().join("baseline_variance.jsonl"),
        &serde_json::to_string(&record).unwrap(),
    );
}

#[test]
fn inline_vec_allocation_variance_is_acceptable() {
    // Measure variance of memory allocation operations
    let rounds = 10;
    let alloc_size = 10_000;
    let mut round_times_us: Vec<f64> = Vec::with_capacity(rounds);

    for _ in 0..rounds {
        let start = std::time::Instant::now();
        for _ in 0..1000 {
            let v: Vec<u64> = (0..alloc_size).collect();
            std::hint::black_box(&v);
        }
        let elapsed_us = start.elapsed().as_micros() as f64;
        round_times_us.push(elapsed_us / 1000.0); // per-iteration
    }

    let stats = compute_variance_stats(&round_times_us).unwrap();

    eprintln!("\n=== Vec Allocation Variance Analysis ===");
    eprintln!("  Rounds:    {rounds}");
    eprintln!("  Mean:      {:.2}us/alloc", stats.mean);
    eprintln!("  CV:        {:.4}", stats.coefficient_of_variation);
    eprintln!("  Class:     {}", stats.variance_class);

    let record = json!({
        "schema": BASELINE_VARIANCE_SCHEMA,
        "metric": "vec_allocation_latency_us",
        "stats": stats,
        "rounds": rounds,
        "acceptable": VarianceClass::from_cv(stats.coefficient_of_variation).is_acceptable(),
    });
    append_jsonl(
        &output_dir().join("baseline_variance.jsonl"),
        &serde_json::to_string(&record).unwrap(),
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// Tests: Capture Baseline Script Validation
// ═══════════════════════════════════════════════════════════════════════════

#[test]
fn capture_baseline_script_exists_and_is_executable() {
    let path = project_root().join("scripts/perf/capture_baseline.sh");
    assert!(
        path.exists(),
        "capture_baseline.sh must exist at scripts/perf/capture_baseline.sh"
    );

    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        let perms = std::fs::metadata(&path).unwrap().permissions();
        assert!(
            perms.mode() & 0o111 != 0,
            "capture_baseline.sh must be executable"
        );
    }
}

#[test]
fn capture_baseline_script_emits_correct_schema() {
    let path = project_root().join("scripts/perf/capture_baseline.sh");
    let content = std::fs::read_to_string(path).unwrap();

    assert!(
        content.contains(BASELINE_VARIANCE_SCHEMA),
        "capture_baseline.sh must emit schema {BASELINE_VARIANCE_SCHEMA}"
    );
}

#[test]
fn capture_baseline_script_supports_required_options() {
    let path = project_root().join("scripts/perf/capture_baseline.sh");
    let content = std::fs::read_to_string(path).unwrap();

    for opt in &["--rounds", "--quick", "--output", "--validate"] {
        assert!(
            content.contains(opt),
            "capture_baseline.sh must support {opt}"
        );
    }
}

#[test]
fn capture_baseline_script_computes_confidence_intervals() {
    let path = project_root().join("scripts/perf/capture_baseline.sh");
    let content = std::fs::read_to_string(path).unwrap();

    assert!(
        content.contains("confidence_interval_95"),
        "must compute 95% confidence intervals"
    );
    assert!(
        content.contains("confidence_interval_99"),
        "must compute 99% confidence intervals"
    );
    assert!(
        content.contains("coefficient_of_variation"),
        "must compute coefficient of variation"
    );
}

#[test]
fn capture_baseline_script_classifies_variance() {
    let path = project_root().join("scripts/perf/capture_baseline.sh");
    let content = std::fs::read_to_string(path).unwrap();

    assert!(
        content.contains("variance_class") || content.contains("var_class"),
        "must classify variance into low/medium/high"
    );
}

#[test]
fn baseline_variance_schema_registered_in_instance() {
    let instance_path = project_root().join("docs/schema/test_evidence_logging_instance.json");
    let content = std::fs::read_to_string(instance_path).unwrap();
    let instance: Value = serde_json::from_str(&content).unwrap();

    let schemas = instance["schema_registry"]["schemas"]
        .as_array()
        .expect("must have schemas array");

    let found = schemas
        .iter()
        .any(|s| s["schema_id"].as_str() == Some(BASELINE_VARIANCE_SCHEMA));
    assert!(
        found,
        "{BASELINE_VARIANCE_SCHEMA} must be registered in the schema instance"
    );
}

#[test]
fn evidence_adjudication_matrix_schema_registered_in_instance() {
    let instance_path = project_root().join("docs/schema/test_evidence_logging_instance.json");
    let content = std::fs::read_to_string(instance_path).unwrap();
    let instance: Value = serde_json::from_str(&content).unwrap();

    let schemas = instance["schema_registry"]["schemas"]
        .as_array()
        .expect("must have schemas array");

    let found = schemas
        .iter()
        .any(|s| s["schema_id"].as_str() == Some(EVIDENCE_ADJUDICATION_MATRIX_SCHEMA));
    assert!(
        found,
        "{EVIDENCE_ADJUDICATION_MATRIX_SCHEMA} must be registered in the schema instance"
    );
}

#[test]
fn evidence_adjudication_matrix_schema_has_core_relationships() {
    let instance_path = project_root().join("docs/schema/test_evidence_logging_instance.json");
    let content = std::fs::read_to_string(instance_path).unwrap();
    let instance: Value = serde_json::from_str(&content).unwrap();

    let relationships = instance["schema_registry"]["schema_relationships"]
        .as_array()
        .expect("must have schema_relationships array");

    let evidence_to_adjudication = relationships.iter().any(|rel| {
        rel["from_schema"] == "pi.qa.evidence_contract.v1"
            && rel["to_schema"] == EVIDENCE_ADJUDICATION_MATRIX_SCHEMA
            && rel["join_field"] == "correlation_id"
    });
    assert!(
        evidence_to_adjudication,
        "schema relationships must link evidence_contract to evidence_adjudication_matrix by correlation_id"
    );

    let run_manifest_to_adjudication = relationships.iter().any(|rel| {
        rel["from_schema"] == "pi.perf.run_manifest.v1"
            && rel["to_schema"] == EVIDENCE_ADJUDICATION_MATRIX_SCHEMA
            && rel["join_field"] == "correlation_id"
    });
    assert!(
        run_manifest_to_adjudication,
        "schema relationships must link run_manifest to evidence_adjudication_matrix by correlation_id"
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// Tests: Progress Claim Validation
// ═══════════════════════════════════════════════════════════════════════════

#[test]
fn claim_requires_sufficient_samples() {
    // A "truthful progress claim" requires at least 5 samples for
    // reasonable confidence intervals
    let too_few = vec![100.0, 105.0];
    let stats = compute_variance_stats(&too_few).unwrap();

    // With only 2 samples, 95% CI should be very wide (t-critical is 12.706 for df=1)
    assert!(
        stats.confidence_interval_95.width_pct > 50.0,
        "2-sample 95% CI should be very wide (>50% of mean), got {:.1}%",
        stats.confidence_interval_95.width_pct
    );

    // With 10 samples, CI should be much tighter
    let enough = vec![
        100.0, 101.0, 99.0, 100.5, 99.5, 100.2, 99.8, 100.1, 99.9, 100.3,
    ];
    let stats10 = compute_variance_stats(&enough).unwrap();
    assert!(
        stats10.confidence_interval_95.width_pct < 5.0,
        "10-sample 95% CI should be tight (<5% of mean), got {:.1}%",
        stats10.confidence_interval_95.width_pct
    );
}

#[test]
fn improvement_claim_requires_non_overlapping_ci() {
    // To claim "X is faster than Y", their 95% CIs should not overlap
    let baseline = vec![
        100.0, 102.0, 98.0, 101.0, 99.0, 100.5, 99.5, 100.2, 99.8, 100.3,
    ];
    let improved = vec![80.0, 82.0, 78.0, 81.0, 79.0, 80.5, 79.5, 80.2, 79.8, 80.3];

    let baseline_stats = compute_variance_stats(&baseline).unwrap();
    let improved_stats = compute_variance_stats(&improved).unwrap();

    // The improved upper bound should be below the baseline lower bound
    let non_overlapping =
        improved_stats.confidence_interval_95.upper < baseline_stats.confidence_interval_95.lower;

    assert!(
        non_overlapping,
        "To claim improvement, improved CI upper ({:.2}) must be < baseline CI lower ({:.2})",
        improved_stats.confidence_interval_95.upper, baseline_stats.confidence_interval_95.lower
    );
}

#[test]
fn marginal_improvement_detected_as_inconclusive() {
    // When CIs overlap, the improvement claim should be marked inconclusive.
    // Use higher-variance data so that a small mean difference produces
    // overlapping confidence intervals — exactly the scenario where a
    // "marginal improvement" claim is statistically unsupported.
    let baseline = vec![95.0, 105.0, 92.0, 108.0, 97.0];
    let marginal = vec![90.0, 102.0, 88.0, 104.0, 93.0];

    let baseline_stats = compute_variance_stats(&baseline).unwrap();
    let marginal_stats = compute_variance_stats(&marginal).unwrap();

    let overlapping =
        marginal_stats.confidence_interval_95.upper >= baseline_stats.confidence_interval_95.lower;

    eprintln!("\n=== Marginal Improvement Detection ===");
    eprintln!(
        "  Baseline 95% CI: [{:.2}, {:.2}]",
        baseline_stats.confidence_interval_95.lower, baseline_stats.confidence_interval_95.upper
    );
    eprintln!(
        "  Marginal 95% CI: [{:.2}, {:.2}]",
        marginal_stats.confidence_interval_95.lower, marginal_stats.confidence_interval_95.upper
    );
    eprintln!("  Overlapping:     {overlapping} (claim is inconclusive)");

    // With ~5% improvement but high variance and only 5 samples,
    // the CIs should overlap, making the claim inconclusive
    assert!(
        overlapping,
        "Small improvement with high variance should be inconclusive"
    );
}

#[test]
fn generate_variance_report() {
    // Summary test that produces a variance analysis report
    let scenarios: Vec<(&str, Vec<f64>)> = vec![
        ("tight_latency", vec![50.0, 50.1, 49.9, 50.05, 49.95]),
        (
            "moderate_latency",
            vec![100.0, 105.0, 95.0, 102.0, 98.0, 107.0, 93.0],
        ),
        ("noisy_latency", vec![100.0, 130.0, 70.0, 150.0, 80.0]),
        (
            "good_throughput",
            vec![
                5000.0, 5100.0, 4900.0, 5050.0, 4950.0, 5020.0, 4980.0, 5010.0, 4990.0, 5030.0,
            ],
        ),
    ];

    let mut report = String::with_capacity(4 * 1024);
    report.push_str("# Baseline Variance Report\n\n");
    report.push_str("| Metric | Samples | Mean | CV | Class | 95% CI Width |\n");
    report.push_str("|---|---|---|---|---|---|\n");

    for (name, samples) in &scenarios {
        let stats = compute_variance_stats(samples).unwrap();
        let _ = writeln!(
            report,
            "| {} | {} | {:.2} | {:.4} | {} | {:.1}% |",
            name,
            stats.count,
            stats.mean,
            stats.coefficient_of_variation,
            stats.variance_class,
            stats.confidence_interval_95.width_pct,
        );

        // Emit JSONL record
        let record = json!({
            "schema": BASELINE_VARIANCE_SCHEMA,
            "metric": name,
            "stats": stats,
            "acceptable": VarianceClass::from_cv(stats.coefficient_of_variation).is_acceptable(),
        });
        append_jsonl(
            &output_dir().join("baseline_variance.jsonl"),
            &serde_json::to_string(&record).unwrap(),
        );
    }

    eprintln!("\n{report}");

    // Write markdown report
    let report_path = output_dir().join("BASELINE_VARIANCE_REPORT.md");
    let _ = std::fs::write(&report_path, &report);
    eprintln!("Report: {}", report_path.display());
}
