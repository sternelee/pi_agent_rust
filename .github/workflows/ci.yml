name: ci

on:
  pull_request:
  push:
    branches: [main]

jobs:
  rust:
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    runs-on: ${{ matrix.os }}
    env:
      CI: "true"
      VCR_MODE: "playback"
      VCR_CASSETTE_DIR: "tests/fixtures/vcr"
      RUST_BACKTRACE: "1"
      CI_GATE_PROMOTION_MODE: ${{ vars.CI_GATE_PROMOTION_MODE || 'strict' }}
      CI_GATE_THRESHOLD_VERSION: ${{ vars.CI_GATE_THRESHOLD_VERSION || '2026-02-08.v1' }}
      CI_GATE_MIN_PASS_RATE_PCT: ${{ vars.CI_GATE_MIN_PASS_RATE_PCT || '80.0' }}
      CI_GATE_MAX_FAIL_COUNT: ${{ vars.CI_GATE_MAX_FAIL_COUNT || '36' }}
      CI_GATE_MAX_NA_COUNT: ${{ vars.CI_GATE_MAX_NA_COUNT || '170' }}
    defaults:
      run:
        working-directory: pi_agent_rust
        shell: bash
    steps:
      - name: Checkout pi_agent_rust
        uses: actions/checkout@v4
        with:
          path: pi_agent_rust

      - name: Install system deps (fd, rg) [linux]
        if: runner.os == 'Linux'
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y fd-find ripgrep
          sudo ln -sf "$(command -v fdfind)" /usr/local/bin/fd

      - name: Install system deps (fd, rg) [macos]
        if: runner.os == 'macOS'
        run: |
          set -euxo pipefail
          brew install fd ripgrep

      - name: Install system deps (fd, rg) [windows]
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          choco install -y fd ripgrep

      - name: Install Rust toolchain (nightly)
        uses: dtolnay/rust-toolchain@nightly
        with:
          components: rustfmt, clippy, llvm-tools-preview

      - name: Install cargo-llvm-cov
        if: runner.os == 'Linux'
        uses: taiki-e/install-action@v2
        with:
          tool: cargo-llvm-cov

      - name: No-mock dependency guard
        run: |
          set -euxo pipefail
          if rg -n --fixed-strings -e mockall -e mockito -e wiremock Cargo.toml Cargo.lock; then
            echo "Mocking crates are forbidden in dependencies."
            exit 1
          fi

      - name: No-mock code guard
        run: |
          set -euxo pipefail

          # Allowlisted exceptions (audited):
          # - MockHttp{Server,Request,Response}: deterministic local TCP server test infra.
          # - MockSpec{,Interceptor}: VCR cassette spec validation + conformance interceptor.
          # - Mocked: comment heading in node_http_shim.rs.
          # - Stubs: string literal in repair event diagnostics (not a mock type).
          allow_re='^(MockHttp(Server|Request|Response)|MockSpec(Interceptor)?|Mocked|Stubs)$'

          matches="$(rg -n --column --no-heading --color never --glob '!tests/ext_conformance/artifacts/**' -o '\b(Mock|Fake|Stub)[A-Za-z0-9_]+\b' tests || true)"
          if [ -z "$matches" ]; then
            exit 0
          fi

          violations="$(echo "$matches" | awk -F: -v allow_re="$allow_re" '$4 !~ allow_re { print }')"
          if [ -n "$violations" ]; then
            echo "$violations"
            echo
            echo "No-mock policy violation: Mock*/Fake*/Stub* identifiers are forbidden in tests."
            echo "Use VCR fixtures or real deps instead. See docs/TEST_COVERAGE_MATRIX.md."
            exit 1
          fi

      - name: Traceability matrix guard
        run: |
          set -euxo pipefail
          if command -v python3 >/dev/null 2>&1; then
            python3 scripts/check_traceability_matrix.py
          elif command -v python >/dev/null 2>&1; then
            python scripts/check_traceability_matrix.py
          else
            echo "python interpreter not found"
            exit 1
          fi

      - name: Suite classification guard
        run: |
          set -euo pipefail
          # Every tests/*.rs file must appear in tests/suite_classification.toml.
          # Subdirectories (common/, provider_streaming/, conformance/, ext_conformance/) are excluded.
          classification="tests/suite_classification.toml"
          if [ ! -f "$classification" ]; then
            echo "Missing $classification — see docs/testing-policy.md"
            exit 1
          fi

          missing=0
          for f in tests/*.rs; do
            stem="$(basename "$f" .rs)"
            if ! rg -q --fixed-strings "\"$stem\"" "$classification"; then
              echo "UNCLASSIFIED: $f is not listed in $classification"
              missing=$((missing + 1))
            fi
          done

          if [ "$missing" -gt 0 ]; then
            echo
            echo "Suite classification violation: $missing test file(s) missing from $classification."
            echo "Add each file to [suite.unit], [suite.vcr], or [suite.e2e]. See docs/testing-policy.md."
            exit 1
          fi
          echo "All test files classified."

      - name: VCR leak guard (unit suite)
        run: |
          set -euo pipefail
          classification="tests/suite_classification.toml"

          # Extract unit suite file names.
          unit_files="$(sed -n '/\[suite\.unit\]/,/\[suite\./{ /^files/,/\]/p }' "$classification" \
            | rg -o '"([^"]+)"' -r '$1' || true)"

          if [ -z "$unit_files" ]; then
            echo "No unit suite files found; skipping VCR leak check."
            exit 0
          fi

          violations=0
          for stem in $unit_files; do
            f="tests/${stem}.rs"
            [ -f "$f" ] || continue
            if rg -q 'VcrRecorder|VcrMode|cassette_root|cassette_dir|fixtures/vcr' "$f"; then
              echo "VCR LEAK: $f is in suite.unit but references VCR infrastructure."
              violations=$((violations + 1))
            fi
          done

          if [ "$violations" -gt 0 ]; then
            echo
            echo "VCR leak violation: $violations unit-suite file(s) reference VCR."
            echo "Move to [suite.vcr] or remove VCR usage. See docs/testing-policy.md."
            exit 1
          fi
          echo "No VCR leaks in unit suite."

      - name: cargo fmt
        run: cargo fmt --check

      - name: cargo clippy
        run: cargo clippy --all-targets -- -D warnings

      - name: cargo clippy (wasm-host) [linux]
        if: runner.os == 'Linux'
        run: cargo clippy --all-targets --features wasm-host -- -D warnings

      - name: cargo doc
        run: cargo doc --no-deps

      - name: cargo test
        run: cargo test --all-targets

      - name: cargo test (wasm-host) [linux]
        if: runner.os == 'Linux'
        run: cargo test --all-targets --features wasm-host

      - name: Unified verification runner (ci profile) [linux]
        if: runner.os == 'Linux'
        run: |
          set -euxo pipefail
          ./scripts/e2e/run_all.sh --profile ci

      - name: CI gate promotion (strict/rollback) [linux]
        if: runner.os == 'Linux'
        run: |
          set -euxo pipefail
          python3 - <<'PY'
          import json
          import os
          import sys
          from datetime import datetime, timezone
          from pathlib import Path


          def parse_float(name: str) -> float:
              raw = os.environ.get(name, "").strip()
              try:
                  return float(raw)
              except ValueError:
                  print(f"invalid {name}: {raw!r}", file=sys.stderr)
                  sys.exit(1)


          def parse_int(name: str) -> int:
              raw = os.environ.get(name, "").strip()
              try:
                  return int(raw)
              except ValueError:
                  print(f"invalid {name}: {raw!r}", file=sys.stderr)
                  sys.exit(1)


          def load_json(path: Path, label: str) -> dict:
              if not path.is_file():
                  raise RuntimeError(f"missing required {label}: {path}")
              with path.open(encoding="utf-8") as handle:
                  payload = json.load(handle)
              if not isinstance(payload, dict):
                  raise RuntimeError(f"invalid {label}: expected JSON object at {path}")
              return payload


          mode = os.environ.get("CI_GATE_PROMOTION_MODE", "strict").strip().lower()
          if mode not in {"strict", "rollback"}:
              print(
                  f"invalid CI_GATE_PROMOTION_MODE={mode!r}; expected 'strict' or 'rollback'",
                  file=sys.stderr,
              )
              sys.exit(1)

          threshold_version = os.environ.get("CI_GATE_THRESHOLD_VERSION", "2026-02-08.v1").strip()
          thresholds = {
              "min_pass_rate_pct": parse_float("CI_GATE_MIN_PASS_RATE_PCT"),
              "max_fail_count": parse_int("CI_GATE_MAX_FAIL_COUNT"),
              "max_na_count": parse_int("CI_GATE_MAX_NA_COUNT"),
          }

          # Rollback semantics are intentionally simple and asserted each run.
          def gate_allows(mode_name: str, has_failures: bool) -> bool:
              return not (has_failures and mode_name == "strict")


          assert gate_allows("strict", False)
          assert not gate_allows("strict", True)
          assert gate_allows("rollback", True)

          summary_candidates = sorted(
              Path("tests/e2e_results").rglob("summary.json"),
              key=lambda path: path.stat().st_mtime,
              reverse=True,
          )
          if not summary_candidates:
              print("no summary.json found under tests/e2e_results", file=sys.stderr)
              sys.exit(1)

          summary_path = summary_candidates[0]
          artifact_dir = summary_path.parent
          evidence_path = artifact_dir / "evidence_contract.json"
          conformance_summary_path = Path("tests/ext_conformance/reports/conformance_summary.json")

          try:
              summary = load_json(summary_path, "summary")
              evidence = load_json(evidence_path, "evidence_contract")
              conformance_summary = load_json(conformance_summary_path, "conformance_summary")
          except RuntimeError as exc:
              print(str(exc), file=sys.stderr)
              sys.exit(1)

          checks = []
          failures = []

          def add_check(check_id: str, actual, threshold, ok: bool) -> None:
              checks.append(
                  {
                      "id": check_id,
                      "actual": actual,
                      "threshold": threshold,
                      "ok": ok,
                  }
              )
              if not ok:
                  failures.append(check_id)


          evidence_schema = evidence.get("schema")
          evidence_status = evidence.get("status")
          evidence_errors = evidence.get("errors")
          if not isinstance(evidence_errors, list):
              evidence_errors = []
          evidence_warnings = evidence.get("warnings")
          if not isinstance(evidence_warnings, list):
              evidence_warnings = []

          add_check(
              "evidence_contract.schema",
              evidence_schema,
              "pi.evidence.contract.v1",
              evidence_schema == "pi.evidence.contract.v1",
          )
          add_check(
              "evidence_contract.status",
              evidence_status,
              "pass",
              evidence_status == "pass",
          )
          add_check(
              "evidence_contract.errors",
              len(evidence_errors),
              0,
              len(evidence_errors) == 0,
          )

          conformance_schema = conformance_summary.get("schema")
          counts = conformance_summary.get("counts")
          if not isinstance(counts, dict):
              counts = {}
          pass_rate_pct = conformance_summary.get("pass_rate_pct")
          fail_count = counts.get("fail")
          na_count = counts.get("na")
          total_count = counts.get("total")

          try:
              pass_rate_pct = float(pass_rate_pct)
          except (TypeError, ValueError):
              pass_rate_pct = None
          try:
              fail_count = int(fail_count)
          except (TypeError, ValueError):
              fail_count = None
          try:
              na_count = int(na_count)
          except (TypeError, ValueError):
              na_count = None
          try:
              total_count = int(total_count)
          except (TypeError, ValueError):
              total_count = None

          evidence_payload = conformance_summary.get("evidence")
          required_evidence_keys = {"golden_fixtures", "load_time_benchmarks", "parity_logs", "smoke_logs"}
          if isinstance(evidence_payload, dict):
              evidence_keys = set(evidence_payload.keys())
          else:
              evidence_keys = set()

          add_check(
              "conformance_summary.schema",
              conformance_schema,
              "pi.ext.conformance_summary.v2",
              conformance_schema == "pi.ext.conformance_summary.v2",
          )
          add_check(
              "conformance_summary.total",
              total_count,
              "> 0",
              total_count is not None and total_count > 0,
          )
          add_check(
              "conformance_summary.pass_rate_pct",
              pass_rate_pct,
              f">= {thresholds['min_pass_rate_pct']}",
              pass_rate_pct is not None and pass_rate_pct >= thresholds["min_pass_rate_pct"],
          )
          add_check(
              "conformance_summary.fail_count",
              fail_count,
              f"<= {thresholds['max_fail_count']}",
              fail_count is not None and fail_count <= thresholds["max_fail_count"],
          )
          add_check(
              "conformance_summary.na_count",
              na_count,
              f"<= {thresholds['max_na_count']}",
              na_count is not None and na_count <= thresholds["max_na_count"],
          )
          add_check(
              "conformance_summary.evidence_keys",
              sorted(evidence_keys),
              sorted(required_evidence_keys),
              required_evidence_keys.issubset(evidence_keys),
          )

          observed = {
              "summary_profile": summary.get("profile"),
              "summary_artifact_dir": summary.get("artifact_dir"),
              "pass_rate_pct": pass_rate_pct,
              "fail_count": fail_count,
              "na_count": na_count,
              "total_count": total_count,
              "evidence_contract_status": evidence_status,
              "evidence_contract_error_count": len(evidence_errors),
              "evidence_contract_warning_count": len(evidence_warnings),
          }

          status = "pass" if not failures else ("rollback_warning" if mode == "rollback" else "fail")
          verdict = {
              "schema": "pi.ci.gate_promotion.v1",
              "generated_at": datetime.now(timezone.utc).isoformat(),
              "threshold_version": threshold_version,
              "mode": mode,
              "status": status,
              "paths": {
                  "summary_json": str(summary_path),
                  "evidence_contract_json": str(evidence_path),
                  "conformance_summary_json": str(conformance_summary_path),
              },
              "thresholds": thresholds,
              "observed": observed,
              "checks": checks,
              "failures": failures,
          }

          verdict_path = artifact_dir / "ci_gate_promotion_v1.json"
          verdict_path.write_text(json.dumps(verdict, indent=2) + "\n", encoding="utf-8")
          print(f"CI gate verdict written: {verdict_path}")

          if failures:
              print("CI gate promotion failures:", file=sys.stderr)
              for check_id in failures:
                  print(f"  - {check_id}", file=sys.stderr)
          if not gate_allows(mode, bool(failures)):
              sys.exit(1)
          PY

      - name: Conformance regression gate [linux]
        if: runner.os == 'Linux'
        env:
          CI_REGRESSION_MODE: ${{ vars.CI_REGRESSION_MODE || 'strict' }}
        run: |
          set -euxo pipefail
          python3 scripts/check_conformance_regression.py

      - name: Release gate (evidence bundle) [linux]
        if: runner.os == 'Linux'
        run: |
          set -euxo pipefail
          ./scripts/release_gate.sh --report | tee release_gate_report.json
          VERDICT=$(python3 -c "import json; print(json.load(open('release_gate_report.json')).get('verdict','unknown'))")
          echo "Release gate verdict: $VERDICT"
          if [ "$VERDICT" = "fail" ]; then
            echo "::warning::Release gate failed — conformance evidence bundle does not meet thresholds"
          fi

      - name: Coverage summary (llvm-cov)
        if: runner.os == 'Linux'
        run: |
          set -euxo pipefail
          start=$(date +%s)
          echo "env: CI=$CI VCR_MODE=$VCR_MODE VCR_CASSETTE_DIR=$VCR_CASSETTE_DIR"
          cargo llvm-cov --all-targets --workspace --summary-only | tee llvm-cov-summary.txt
          end=$(date +%s)
          echo "coverage_summary_duration_s=$((end-start))"

      - name: Coverage gate (lcov)
        if: runner.os == 'Linux'
        run: |
          set -euxo pipefail
          start=$(date +%s)
          # Coverage gate: 50% (actual: ~66% as of 2026-02-05)
          cargo llvm-cov --all-targets --workspace --lcov --output-path lcov.info --fail-under-lines 50
          ls -lah lcov.info
          end=$(date +%s)
          echo "coverage_lcov_duration_s=$((end-start))"

      - name: Coverage report (html)
        if: runner.os == 'Linux'
        run: cargo llvm-cov --all-targets --workspace --html

      - name: Upload coverage artifacts
        if: runner.os == 'Linux'
        uses: actions/upload-artifact@v4
        with:
          name: coverage
          path: |
            pi_agent_rust/llvm-cov-summary.txt
            pi_agent_rust/lcov.info
            pi_agent_rust/target/llvm-cov/html

      - name: Upload conformance reports
        if: runner.os == 'Linux' && always()
        uses: actions/upload-artifact@v4
        with:
          name: conformance-reports
          path: |
            pi_agent_rust/tests/ext_conformance/reports/**/*.json
            pi_agent_rust/tests/ext_conformance/reports/**/*.jsonl
            pi_agent_rust/tests/e2e_results/**/*.json
            pi_agent_rust/tests/e2e_results/**/*.jsonl
            pi_agent_rust/tests/e2e_results/**/*.log
            pi_agent_rust/release_gate_report.json
          if-no-files-found: ignore
